{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6080f8bb",
   "metadata": {},
   "source": [
    "# 1. Spark Session + Crear Dataframe + Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3da862d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/28 20:05:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----+------+----------+----------+\n",
      "|Nombre|      Apellido|Edad|  País|    Estado| Profesión|\n",
      "+------+--------------+----+------+----------+----------+\n",
      "|Victor|Manuel Rámirez|  22|México|       SLP| Ingeniero|\n",
      "|  Juan|         Pérez|  30|México|      CDMX|    Doctor|\n",
      "|   Ana|        García|  25|México|   Jalisco|   Abogada|\n",
      "|  Luis|     Hernández|  28|México|Nuevo León|Arquitecto|\n",
      "| María|         López|  35|México|    Puebla|   Maestra|\n",
      "| Pedro|      Martínez|  40|México|Guanajuato|  Contador|\n",
      "+------+--------------+----+------+----------+----------+\n",
      "\n",
      "+------+--------+----+------+----------+---------+\n",
      "|Nombre|Apellido|Edad|  País|    Estado|Profesión|\n",
      "+------+--------+----+------+----------+---------+\n",
      "| María|   López|  35|México|    Puebla|  Maestra|\n",
      "| Pedro|Martínez|  40|México|Guanajuato| Contador|\n",
      "+------+--------+----+------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Importar la clase SparkSession desde la librería pyspark.sql\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 2. Crear una instancia de SparkSession, que es el punto de entrada a la funcionalidad de Spark.\n",
    "# .appName(\"Test\") le da un nombre a la aplicación.\n",
    "# .getOrCreate() obtiene una sesión existente o crea una nueva.\n",
    "spark = SparkSession.builder.appName(\"Test\").getOrCreate()\n",
    "\n",
    "# 3. Definir los datos que se usarán para el DataFrame.\n",
    "# Es una lista de tuplas, donde cada tupla representa una fila.\n",
    "data = [(\"Victor\", \"Manuel Rámirez\", 22, \"México\", \"SLP\", \"Ingeniero\"),\n",
    "        (\"Juan\", \"Pérez\", 30, \"México\", \"CDMX\", \"Doctor\"),\n",
    "        (\"Ana\", \"García\", 25, \"México\", \"Jalisco\", \"Abogada\"),\n",
    "        (\"Luis\", \"Hernández\", 28, \"México\", \"Nuevo León\", \"Arquitecto\"),\n",
    "        (\"María\", \"López\", 35, \"México\", \"Puebla\", \"Maestra\"),\n",
    "        (\"Pedro\", \"Martínez\", 40, \"México\", \"Guanajuato\", \"Contador\")]\n",
    "\n",
    "# 4. Definir los nombres de las columnas para el DataFrame.\n",
    "# El orden corresponde al de los datos en las tuplas.\n",
    "columns = [\"Nombre\", \"Apellido\", \"Edad\", \"País\", \"Estado\", \"Profesión\"]\n",
    "\n",
    "# 5. Crear el DataFrame a partir de los datos y las columnas definidas.\n",
    "# Un DataFrame es una estructura de datos distribuida similar a una tabla.\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# 6. Mostrar las primeras 20 filas del DataFrame para verificar su contenido.\n",
    "# Esta es una acción que ejecuta las transformaciones previas.\n",
    "df.show()\n",
    "\n",
    "# 7. Aplicar una transformación de filtro para crear un nuevo DataFrame.\n",
    "# La condición es seleccionar solo las filas donde la columna \"Edad\" sea mayor a 30.\n",
    "df_edad = df.filter(df.Edad > 30)\n",
    "\n",
    "# 8. Mostrar el DataFrame resultante después del filtrado.\n",
    "df_edad.show()\n",
    "\n",
    "# 9. Detener la sesión de Spark para liberar los recursos del clúster.\n",
    "# Es una buena práctica hacerlo al final de cada script.\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7776a181",
   "metadata": {},
   "source": [
    "# 2. GroupBy + Aggregate (agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffc7c72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Original:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----+------+----------+----------+\n",
      "|Nombre|      Apellido|Edad|  País|    Estado| Profesión|\n",
      "+------+--------------+----+------+----------+----------+\n",
      "|Victor|Manuel Rámirez|  22|México|       SLP| Ingeniero|\n",
      "|  Juan|         Pérez|  30|México|      CDMX|    Doctor|\n",
      "|   Ana|        García|  25|México|   Jalisco|   Abogada|\n",
      "|  Luis|     Hernández|  28|México|Nuevo León|Arquitecto|\n",
      "| María|         López|  35|México|    Puebla|   Maestra|\n",
      "| Pedro|      Martínez|  40|México|Guanajuato|  Contador|\n",
      "+------+--------------+----+------+----------+----------+\n",
      "\n",
      "DataFrame Agregado por Estado:\n",
      "+----------+---------------+-------------+-----------+\n",
      "|    Estado|numero_personas|edad_promedio|edad_maxima|\n",
      "+----------+---------------+-------------+-----------+\n",
      "|       SLP|              1|         22.0|         22|\n",
      "|      CDMX|              1|         30.0|         30|\n",
      "|   Jalisco|              1|         25.0|         25|\n",
      "|Nuevo León|              1|         28.0|         28|\n",
      "|    Puebla|              1|         35.0|         35|\n",
      "|Guanajuato|              1|         40.0|         40|\n",
      "+----------+---------------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Importar funciones específicas para realizar agregaciones desde pyspark.sql.functions\n",
    "from pyspark.sql.functions import avg, count, max\n",
    "\n",
    "# Importar la clase SparkSession para iniciar la sesión\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 2. Crear o reutilizar una sesión de Spark con el nombre \"Agregaciones\"\n",
    "spark = SparkSession.builder.appName(\"Agregaciones\").getOrCreate()\n",
    "\n",
    "# 3. Definir los datos de ejemplo en una lista de tuplas\n",
    "data = [(\"Victor\", \"Manuel Rámirez\", 22, \"México\", \"SLP\", \"Ingeniero\"),\n",
    "        (\"Juan\", \"Pérez\", 30, \"México\", \"CDMX\", \"Doctor\"),\n",
    "        (\"Ana\", \"García\", 25, \"México\", \"Jalisco\", \"Abogada\"),\n",
    "        (\"Luis\", \"Hernández\", 28, \"México\", \"Nuevo León\", \"Arquitecto\"),\n",
    "        (\"María\", \"López\", 35, \"México\", \"Puebla\", \"Maestra\"),\n",
    "        (\"Pedro\", \"Martínez\", 40, \"México\", \"Guanajuato\", \"Contador\")]\n",
    "\n",
    "# Definir los nombres de las columnas correspondientes\n",
    "columns = [\"Nombre\", \"Apellido\", \"Edad\", \"País\", \"Estado\", \"Profesión\"]\n",
    "\n",
    "# Crear el DataFrame inicial a partir de los datos y columnas\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Mostrar el DataFrame original para verificarlo\n",
    "print(\"DataFrame Original:\")\n",
    "df.show()\n",
    "\n",
    "# 4. Agrupar el DataFrame y aplicar funciones de agregación\n",
    "# df.groupBy(\"Estado\") agrupa todas las filas por los valores únicos de la columna \"Estado\".\n",
    "# .agg() aplica una o más funciones de agregación a cada uno de esos grupos.\n",
    "df_agregado = df.groupBy(\"Estado\").agg(\n",
    "    # count(\"*\") cuenta el número de filas en cada grupo.\n",
    "    # .alias() renombra la nueva columna a \"numero_personas\".\n",
    "    count(\"*\").alias(\"numero_personas\"),\n",
    "    \n",
    "    # avg(\"Edad\") calcula el promedio de la columna \"Edad\" para cada grupo.\n",
    "    # Se renombra la columna resultante a \"edad_promedio\".\n",
    "    avg(\"Edad\").alias(\"edad_promedio\"),\n",
    "    \n",
    "    # max(\"Edad\") encuentra el valor máximo de la columna \"Edad\" en cada grupo.\n",
    "    # Se renombra la columna a \"edad_maxima\".\n",
    "    max(\"Edad\").alias(\"edad_maxima\")\n",
    ")\n",
    "\n",
    "# 5. Mostrar el DataFrame resultante con los datos agregados\n",
    "print(\"DataFrame Agregado por Estado:\")\n",
    "df_agregado.show()\n",
    "\n",
    "# 6. Detener la sesión de Spark para liberar los recursos\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc707d2",
   "metadata": {},
   "source": [
    "# 3. Leer archivos CSV + InferSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22201bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estructura del DataFrame leído del CSV:\n",
      "root\n",
      " |-- producto_id: integer (nullable = true)\n",
      " |-- nombre_producto: string (nullable = true)\n",
      " |-- categoria: string (nullable = true)\n",
      " |-- precio: double (nullable = true)\n",
      " |-- stock: integer (nullable = true)\n",
      "\n",
      "Contenido del DataFrame de productos:\n",
      "+-----------+----------------+-----------+------+-----+\n",
      "|producto_id| nombre_producto|  categoria|precio|stock|\n",
      "+-----------+----------------+-----------+------+-----+\n",
      "|        101|      Laptop Pro|Electronica|1200.0|   50|\n",
      "|        102|     Silla Gamer|    Muebles| 350.5|  120|\n",
      "|        103|Teclado Mecanico|Electronica|150.75|  200|\n",
      "|        104| Mesa de Oficina|    Muebles| 250.0|   80|\n",
      "|        105|      Monitor 4K|Electronica| 800.0|   75|\n",
      "+-----------+----------------+-----------+------+-----+\n",
      "\n",
      "Productos de la categoría 'Electronica' con más de 100 en stock:\n",
      "+-----------+----------------+-----------+------+-----+\n",
      "|producto_id| nombre_producto|  categoria|precio|stock|\n",
      "+-----------+----------------+-----------+------+-----+\n",
      "|        103|Teclado Mecanico|Electronica|150.75|  200|\n",
      "+-----------+----------------+-----------+------+-----+\n",
      "\n",
      "Productos de la categoría 'Muebles' con más de 100 en stock:\n",
      "+-----------+---------------+---------+------+-----+\n",
      "|producto_id|nombre_producto|categoria|precio|stock|\n",
      "+-----------+---------------+---------+------+-----+\n",
      "|        102|    Silla Gamer|  Muebles| 350.5|  120|\n",
      "+-----------+---------------+---------+------+-----+\n",
      "\n",
      "\n",
      "Resumen de datos por categoría:\n",
      "+-----------+----------------+-----------------+------------+\n",
      "|  categoria|numero_productos|  precio_promedio|stock_maximo|\n",
      "+-----------+----------------+-----------------+------------+\n",
      "|Electronica|               3|716.9166666666666|         200|\n",
      "|    Muebles|               2|           300.25|         120|\n",
      "+-----------+----------------+-----------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Importaciones Necesarias ---\n",
    "from pyspark.sql import SparkSession\n",
    "# Importamos las funciones que usaremos para las agregaciones\n",
    "from pyspark.sql.functions import count, avg, max\n",
    "\n",
    "# --- 2. Creación de la SparkSession ---\n",
    "# Es el punto de entrada para cualquier aplicación de Spark\n",
    "spark = SparkSession.builder.appName(\"LecturaArchivos\").getOrCreate()\n",
    "\n",
    "# --- 3. Lectura de un Archivo CSV ---\n",
    "# spark.read es la interfaz para leer datos de fuentes externas.\n",
    "# .csv() especifica que el formato del archivo es CSV.\n",
    "df_productos = spark.read.csv(\n",
    "    \"datos.csv\",          # Ruta del archivo a leer\n",
    "    header=True,          # La primera fila del CSV se usará como encabezado (nombres de columna)\n",
    "    inferSchema=True      # Spark intentará adivinar el tipo de dato de cada columna (Integer, String, etc.)\n",
    ")\n",
    "\n",
    "# --- 4. Inspección Inicial del DataFrame ---\n",
    "# .printSchema() muestra la estructura del DataFrame: nombres de columna y sus tipos de datos.\n",
    "print(\"Estructura del DataFrame leído del CSV:\")\n",
    "df_productos.printSchema()\n",
    "\n",
    "# .show() muestra las primeras 20 filas del DataFrame.\n",
    "print(\"Contenido del DataFrame de productos:\")\n",
    "df_productos.show()\n",
    "\n",
    "# --- 5. Filtrado con Múltiples Condiciones ---\n",
    "# Se filtran los productos que cumplen dos condiciones a la vez.\n",
    "# Nota: Cada condición debe ir entre paréntesis y se unen con '&' (AND) o '|' (OR).\n",
    "print(\"Productos de la categoría 'Electronica' con más de 100 en stock:\")\n",
    "df_productos.filter(\n",
    "    (df_productos.categoria == \"Electronica\") & (df_productos.stock > 100)\n",
    ").show()\n",
    "    \n",
    "print(\"Productos de la categoría 'Muebles' con más de 100 en stock:\")\n",
    "df_productos.filter(\n",
    "    (df_productos.categoria == \"Muebles\") & (df_productos.stock > 100)\n",
    ").show()\n",
    "\n",
    "# --- 6. Agregación de Datos por Categoría ---\n",
    "# .groupBy() agrupa las filas según una columna para poder aplicar cálculos a cada grupo.\n",
    "# .agg() ejecuta las funciones de agregación sobre los grupos.\n",
    "print(\"\\nResumen de datos por categoría:\")\n",
    "df_productos.groupBy(\"categoria\").agg(\n",
    "    count(\"*\").alias(\"numero_productos\"), # Contar cuántos productos hay en cada categoría\n",
    "    avg(\"precio\").alias(\"precio_promedio\"),   # Calcular el precio promedio por categoría\n",
    "    max(\"stock\").alias(\"stock_maximo\")        # Encontrar el stock máximo por categoría\n",
    ").show()\n",
    "\n",
    "# --- 7. Finalización de la Sesión ---\n",
    "# Es una buena práctica detener la sesión para liberar los recursos.\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618177e8",
   "metadata": {},
   "source": [
    "# 4. Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1100292a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empleados:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------------+\n",
      "| id|nombre|departamento_id|\n",
      "+---+------+---------------+\n",
      "|  1|   Ana|              3|\n",
      "|  2|  Luis|              1|\n",
      "|  3| Marta|              2|\n",
      "|  4|  Juan|              3|\n",
      "|  5| Sofia|           NULL|\n",
      "+---+------+---------------+\n",
      "\n",
      "Departamentos:\n",
      "+---+----------------+\n",
      "| id|    nombre_depto|\n",
      "+---+----------------+\n",
      "|  1|          Ventas|\n",
      "|  2|       Marketing|\n",
      "|  3|      Ingeniería|\n",
      "|  4|Recursos Humanos|\n",
      "+---+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Importaciones y Creación de la Sesión ---\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Crear o reutilizar una sesión de Spark con el nombre \"EjemploJoins\"\n",
    "spark = SparkSession.builder.appName(\"EjemploJoins\").getOrCreate()\n",
    "\n",
    "# --- 2. Creación de los DataFrames de Ejemplo ---\n",
    "\n",
    "# DataFrame de Empleados: Contiene ID de empleado, nombre y el ID del departamento al que pertenece.\n",
    "datos_empleados = [\n",
    "    (1, \"Ana\", 3),\n",
    "    (2, \"Luis\", 1),\n",
    "    (3, \"Marta\", 2),\n",
    "    (4, \"Juan\", 3),\n",
    "    (5, \"Sofia\", None) # Sofia no tiene un departamento asignado (valor nulo)\n",
    "]\n",
    "df_empleados = spark.createDataFrame(datos_empleados, [\"id\", \"nombre\", \"departamento_id\"])\n",
    "\n",
    "# DataFrame de Departamentos: Contiene el ID del departamento y su nombre.\n",
    "datos_deptos = [\n",
    "    (1, \"Ventas\"),\n",
    "    (2, \"Marketing\"),\n",
    "    (3, \"Ingeniería\"),\n",
    "    (4, \"Recursos Humanos\") # El depto. de RRHH no tiene empleados asignados en nuestra tabla de empleados.\n",
    "]\n",
    "df_deptos = spark.createDataFrame(datos_deptos, [\"id\", \"nombre_depto\"])\n",
    "\n",
    "# --- 3. Mostrar los DataFrames Originales ---\n",
    "print(\"Empleados:\")\n",
    "df_empleados.show()\n",
    "\n",
    "print(\"Departamentos:\")\n",
    "df_deptos.show()\n",
    "\n",
    "# --- 4. Unir (Join) los dos DataFrames ---\n",
    "# La operación 'join' combina filas de dos DataFrames basándose en una condición de igualdad.\n",
    "# Sintaxis: df1.join(df2, condición_de_unión, tipo_de_join)\n",
    "df_unido = df_empleados.join(\n",
    "    df_deptos,                                 # El segundo DataFrame a unir.\n",
    "    df_empleados.departamento_id == df_deptos.id, # La condición que conecta las dos tablas.\n",
    "    \"inner\"                                    # Tipo de join. \"inner\" solo incluye filas donde la clave existe en AMBAS tablas.\n",
    ")\n",
    "# En un 'inner join':\n",
    "# - 'Sofia' será"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cce1da0",
   "metadata": {},
   "source": [
    "# 5. Write file as parquet & csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c7d84d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame final que vamos a guardar:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/28 20:06:04 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------------+------------+\n",
      "| id|nombre|departamento_id|nombre_depto|\n",
      "+---+------+---------------+------------+\n",
      "|  2|  Luis|              1|      Ventas|\n",
      "|  3| Marta|              2|   Marketing|\n",
      "|  1|   Ana|              3|  Ingeniería|\n",
      "|  4|  Juan|              3|  Ingeniería|\n",
      "+---+------+---------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡DataFrames guardados exitosamente!\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Importaciones y Creación de la Sesión ---\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"EscrituraArchivos\").getOrCreate()\n",
    "\n",
    "# --- 2. Preparación de Datos ---\n",
    "# Se recrean los DataFrames de empleados y departamentos.\n",
    "datos_empleados = [(1, \"Ana\", 3), (2, \"Luis\", 1), (3, \"Marta\", 2), (4, \"Juan\", 3)]\n",
    "df_empleados = spark.createDataFrame(datos_empleados, [\"id\", \"nombre\", \"departamento_id\"])\n",
    "\n",
    "datos_deptos = [(1, \"Ventas\"), (2, \"Marketing\"), (3, \"Ingeniería\")]\n",
    "df_deptos = spark.createDataFrame(datos_deptos, [\"id\", \"nombre_depto\"])\n",
    "\n",
    "# Se realiza el 'join' para combinar los datos.\n",
    "df_unido = df_empleados.join(df_deptos, df_empleados.departamento_id == df_deptos.id, \"inner\")\n",
    "\n",
    "# --- 3. Limpieza del DataFrame ---\n",
    "# Después de un join, es común tener columnas de ID duplicadas.\n",
    "# Se elimina la columna 'id' que viene del DataFrame de departamentos para evitar redundancia.\n",
    "df_unido = df_unido.drop(df_deptos.id)\n",
    "print(\"DataFrame final que vamos a guardar:\")\n",
    "df_unido.show()\n",
    "\n",
    "# --- 4. Escritura de Archivos ---\n",
    "# 'df.write' es la interfaz para guardar un DataFrame en un sistema de archivos.\n",
    "\n",
    "# --- Guardar como Parquet ---\n",
    "# Parquet es un formato de archivo columnar muy eficiente para análisis de Big Data.\n",
    "df_unido.write.mode(\"overwrite\").parquet(\"salida_parquet\")\n",
    "# .mode() especifica el comportamiento si el archivo/directorio ya existe:\n",
    "#  - \"overwrite\": Reemplaza los datos existentes.\n",
    "#  - \"append\": Agrega los nuevos datos a los existentes.\n",
    "#  - \"ignore\": No hace nada si ya existen datos.\n",
    "#  - \"error\" (o \"errorifexists\"): Lanza un error (comportamiento por defecto).\n",
    "\n",
    "# --- Guardar como CSV ---\n",
    "# CSV es un formato de texto plano, útil por su compatibilidad.\n",
    "df_unido.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"salida_csv\")\n",
    "# .option(\"header\", \"true\") es específico para CSV e indica que se debe escribir\n",
    "# una primera fila con los nombres de las columnas.\n",
    "\n",
    "print(\"¡DataFrames guardados exitosamente!\")\n",
    "\n",
    "# --- 5. Finalización de la Sesión ---\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2c5465",
   "metadata": {},
   "source": [
    "# 6. UDF & withColumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2222d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame con la categoría de experiencia:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----------+\n",
      "|nombre|edad|experiencia|\n",
      "+------+----+-----------+\n",
      "|   Ana|  28|Semi-Senior|\n",
      "|  Luis|  35|Semi-Senior|\n",
      "| Marta|  42|     Senior|\n",
      "|  Juan|  22|     Junior|\n",
      "| Sofia|  55|     Senior|\n",
      "+------+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Importaciones Necesarias ---\n",
    "from pyspark.sql import SparkSession\n",
    "# udf: Función para registrar una función de Python como una UDF de Spark.\n",
    "from pyspark.sql.functions import udf\n",
    "# StringType: Tipo de dato para especificar qué devolverá nuestra UDF.\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# --- 2. Creación de la SparkSession ---\n",
    "spark = SparkSession.builder.appName(\"EjemploUDF\").getOrCreate()\n",
    "\n",
    "# --- 3. Creación del DataFrame de Ejemplo ---\n",
    "datos = [(\"Ana\", 28),\n",
    "         (\"Luis\", 35),\n",
    "         (\"Marta\", 42),\n",
    "         (\"Juan\", 22),\n",
    "         (\"Sofia\", 55)]\n",
    "df = spark.createDataFrame(datos, [\"nombre\", \"edad\"])\n",
    "\n",
    "# --- 4. Definición y Registro de una UDF ---\n",
    "\n",
    "# Paso 1: Definimos una función de Python normal.\n",
    "# Esta función toma un valor (la edad) y devuelve una categoría como un string.\n",
    "def categorizar_experiencia(edad):\n",
    "    if edad < 25:\n",
    "        return \"Junior\"\n",
    "    elif 25 <= edad < 40:\n",
    "        return \"Semi-Senior\"\n",
    "    else:\n",
    "        return \"Senior\"\n",
    "\n",
    "# Paso 2: Registramos la función de Python como una UDF de Spark.\n",
    "# Le decimos a Spark cuál función usar (categorizar_experiencia) y\n",
    "# qué tipo de dato va a devolver (StringType). Esto es crucial.\n",
    "experiencia_udf = udf(categorizar_experiencia, StringType())\n",
    "\n",
    "# --- 5. Aplicación de la UDF para Crear una Nueva Columna ---\n",
    "\n",
    "# Paso 3: Usamos .withColumn para agregar una nueva columna llamada \"experiencia\".\n",
    "# Para generar los valores de la nueva columna, llamamos a nuestra UDF\n",
    "# y le pasamos la columna 'edad' del DataFrame como argumento.\n",
    "df_con_experiencia = df.withColumn(\n",
    "    \"experiencia\",\n",
    "    experiencia_udf(df.edad)\n",
    ")\n",
    "\n",
    "# --- 6. Visualización y Finalización ---\n",
    "print(\"DataFrame con la categoría de experiencia:\")\n",
    "df_con_experiencia.show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28323d97",
   "metadata": {},
   "source": [
    "# 7. Spark UI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "710640c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 destinos desde JFK:\n",
      "+----+------------+\n",
      "|dest|total_vuelos|\n",
      "+----+------------+\n",
      "| LAX|           3|\n",
      "| ORD|           2|\n",
      "| MIA|           1|\n",
      "| SFO|           1|\n",
      "+----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Importaciones y Creación de la Sesión ---\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Iniciar la sesión de Spark. El nombre \"ExplorandoSparkUI\" te ayudará a identificarla.\n",
    "spark = SparkSession.builder.appName(\"ExplorandoSparkUI\").getOrCreate()\n",
    "\n",
    "# --- 2. Definición de Transformaciones (Operaciones Perezosas) ---\n",
    "# Spark no ejecuta nada todavía, solo construye un plan (DAG) de lo que tiene que hacer.\n",
    "\n",
    "# Leer un archivo CSV de vuelos. Asegúrate de tener un archivo con este nombre.\n",
    "df_vuelos = spark.read.option(\"header\", \"true\").csv(\"vuelos.csv\")\n",
    "\n",
    "# Filtrar para quedarnos solo con los vuelos que originan en \"JFK\".\n",
    "df_vuelos = df_vuelos.filter(df_vuelos.origin == \"JFK\")\n",
    "\n",
    "# Agrupar por destino y contar el número de vuelos para cada uno.\n",
    "df_agrupado = df_vuelos.groupBy(\"dest\").agg(count(\"*\").alias(\"total_vuelos\"))\n",
    "\n",
    "# Ordenar los resultados para ver los destinos más populares primero.\n",
    "df_ordenado = df_agrupado.orderBy(\"total_vuelos\", ascending=False)\n",
    "\n",
    "# --- 3. Ejecución de una Acción (Lanzamiento del Job) ---\n",
    "# La llamada a .show() es una \"acción\". Obliga a Spark a ejecutar el plan de transformaciones.\n",
    "# En este momento se lanza un \"Job\" de Spark, que podrás ver en la Spark UI.\n",
    "print(\"Top 5 destinos desde JFK:\")\n",
    "df_ordenado.show(5)\n",
    "\n",
    "# --- 4. Pausa para Explorar la Spark UI ---\n",
    "# La Spark UI normalmente solo está activa mientras el script se ejecuta.\n",
    "# Este 'input()' pausa el script, manteniendo la sesión y la UI activas.\n",
    "# Abre tu navegador y ve a http://localhost:4040 para explorar los detalles del Job.\n",
    "input(\"Presiona Enter en la terminal para terminar el script y cerrar la Spark UI...\")\n",
    "\n",
    "# --- 5. Detención de la Sesión ---\n",
    "# Una vez que presionas Enter, el script continúa y detiene la sesión.\n",
    "# Esto también apaga la Spark UI.\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957a41cc",
   "metadata": {},
   "source": [
    "# 8. Lectura y escritura de archivos JSON -> parquet/csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b14099b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esquema inferido del JSON:\n",
      "root\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- items: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- product_id: string (nullable = true)\n",
      " |    |    |-- quantity: long (nullable = true)\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- shipping_address: struct (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      " |    |-- street: string (nullable = true)\n",
      "\n",
      "+----------------+----------------------+--------+--------------------------------------+\n",
      "|customer_name   |items                 |order_id|shipping_address                      |\n",
      "+----------------+----------------------+--------+--------------------------------------+\n",
      "|Carlos Ruiz     |[{P001, 2}, {P002, 1}]|101     |{Springfield, Calle Falsa 123}        |\n",
      "|Ana Lopez       |[{P003, 5}]           |102     |{Shelbyville, Avenida Siempreviva 742}|\n",
      "|Luis Gomez      |[{P001, 1}, {P004, 3}]|103     |{Springfield, Calle del Sol 456}      |\n",
      "|Maria Perez     |[{P002, 2}, {P005, 1}]|104     |{Shelbyville, Plaza Mayor 789}        |\n",
      "|Juan Martinez   |[{P003, 4}, {P006, 2}]|105     |{Springfield, Calle Luna 321}         |\n",
      "|Elena Torres    |[{P001, 2}, {P007, 1}]|106     |{Shelbyville, Calle Estrella 654}     |\n",
      "|Pedro Sanchez   |[{P004, 1}, {P005, 3}]|107     |{Springfield, Calle Mar 987}          |\n",
      "|Laura Jimenez   |[{P002, 1}, {P006, 2}]|108     |{Shelbyville, Avenida del Parque 111} |\n",
      "|Jorge Ramirez   |[{P003, 3}, {P007, 2}]|109     |{Springfield, Calle Rio 222}          |\n",
      "|Sofia Alvarez   |[{P001, 4}, {P005, 1}]|110     |{Shelbyville, Plaza Central 333}      |\n",
      "|Ricardo Torres  |[{P002, 3}, {P006, 1}]|111     |{Springfield, Calle Norte 444}        |\n",
      "|Claudia Morales |[{P004, 2}, {P007, 3}]|112     |{Shelbyville, Avenida Sur 555}        |\n",
      "|Andres Castillo |[{P001, 1}, {P002, 2}]|113     |{Springfield, Calle Este 666}         |\n",
      "|Isabel Fernandez|[{P003, 2}, {P005, 4}]|114     |{Shelbyville, Plaza Oeste 777}        |\n",
      "|Fernando Ruiz   |[{P006, 3}, {P007, 1}]|115     |{Springfield, Calle Central 888}      |\n",
      "|Patricia Gomez  |[{P002, 5}, {P004, 2}]|116     |{Shelbyville, Avenida Norte 999}      |\n",
      "+----------------+----------------------+--------+--------------------------------------+\n",
      "\n",
      "Accediendo a un campo anidado (ciudad):\n",
      "+--------+-----------+\n",
      "|order_id|       city|\n",
      "+--------+-----------+\n",
      "|     101|Springfield|\n",
      "|     102|Shelbyville|\n",
      "|     103|Springfield|\n",
      "|     104|Shelbyville|\n",
      "|     105|Springfield|\n",
      "|     106|Shelbyville|\n",
      "|     107|Springfield|\n",
      "|     108|Shelbyville|\n",
      "|     109|Springfield|\n",
      "|     110|Shelbyville|\n",
      "|     111|Springfield|\n",
      "|     112|Shelbyville|\n",
      "|     113|Springfield|\n",
      "|     114|Shelbyville|\n",
      "|     115|Springfield|\n",
      "|     116|Shelbyville|\n",
      "+--------+-----------+\n",
      "\n",
      "DataFrame después de 'explode' (una fila por cada item del pedido):\n",
      "+--------------+----------------------+--------+--------------------------------------+---------+\n",
      "|customer_name |items                 |order_id|shipping_address                      |item     |\n",
      "+--------------+----------------------+--------+--------------------------------------+---------+\n",
      "|Carlos Ruiz   |[{P001, 2}, {P002, 1}]|101     |{Springfield, Calle Falsa 123}        |{P001, 2}|\n",
      "|Carlos Ruiz   |[{P001, 2}, {P002, 1}]|101     |{Springfield, Calle Falsa 123}        |{P002, 1}|\n",
      "|Ana Lopez     |[{P003, 5}]           |102     |{Shelbyville, Avenida Siempreviva 742}|{P003, 5}|\n",
      "|Luis Gomez    |[{P001, 1}, {P004, 3}]|103     |{Springfield, Calle del Sol 456}      |{P001, 1}|\n",
      "|Luis Gomez    |[{P001, 1}, {P004, 3}]|103     |{Springfield, Calle del Sol 456}      |{P004, 3}|\n",
      "|Maria Perez   |[{P002, 2}, {P005, 1}]|104     |{Shelbyville, Plaza Mayor 789}        |{P002, 2}|\n",
      "|Maria Perez   |[{P002, 2}, {P005, 1}]|104     |{Shelbyville, Plaza Mayor 789}        |{P005, 1}|\n",
      "|Juan Martinez |[{P003, 4}, {P006, 2}]|105     |{Springfield, Calle Luna 321}         |{P003, 4}|\n",
      "|Juan Martinez |[{P003, 4}, {P006, 2}]|105     |{Springfield, Calle Luna 321}         |{P006, 2}|\n",
      "|Elena Torres  |[{P001, 2}, {P007, 1}]|106     |{Shelbyville, Calle Estrella 654}     |{P001, 2}|\n",
      "|Elena Torres  |[{P001, 2}, {P007, 1}]|106     |{Shelbyville, Calle Estrella 654}     |{P007, 1}|\n",
      "|Pedro Sanchez |[{P004, 1}, {P005, 3}]|107     |{Springfield, Calle Mar 987}          |{P004, 1}|\n",
      "|Pedro Sanchez |[{P004, 1}, {P005, 3}]|107     |{Springfield, Calle Mar 987}          |{P005, 3}|\n",
      "|Laura Jimenez |[{P002, 1}, {P006, 2}]|108     |{Shelbyville, Avenida del Parque 111} |{P002, 1}|\n",
      "|Laura Jimenez |[{P002, 1}, {P006, 2}]|108     |{Shelbyville, Avenida del Parque 111} |{P006, 2}|\n",
      "|Jorge Ramirez |[{P003, 3}, {P007, 2}]|109     |{Springfield, Calle Rio 222}          |{P003, 3}|\n",
      "|Jorge Ramirez |[{P003, 3}, {P007, 2}]|109     |{Springfield, Calle Rio 222}          |{P007, 2}|\n",
      "|Sofia Alvarez |[{P001, 4}, {P005, 1}]|110     |{Shelbyville, Plaza Central 333}      |{P001, 4}|\n",
      "|Sofia Alvarez |[{P001, 4}, {P005, 1}]|110     |{Shelbyville, Plaza Central 333}      |{P005, 1}|\n",
      "|Ricardo Torres|[{P002, 3}, {P006, 1}]|111     |{Springfield, Calle Norte 444}        |{P002, 3}|\n",
      "+--------------+----------------------+--------+--------------------------------------+---------+\n",
      "only showing top 20 rows\n",
      "Resultado final y aplanado:\n",
      "+--------+--------------+-----------+----------+--------+\n",
      "|order_id| customer_name|       city|product_id|quantity|\n",
      "+--------+--------------+-----------+----------+--------+\n",
      "|     101|   Carlos Ruiz|Springfield|      P001|       2|\n",
      "|     101|   Carlos Ruiz|Springfield|      P002|       1|\n",
      "|     102|     Ana Lopez|Shelbyville|      P003|       5|\n",
      "|     103|    Luis Gomez|Springfield|      P001|       1|\n",
      "|     103|    Luis Gomez|Springfield|      P004|       3|\n",
      "|     104|   Maria Perez|Shelbyville|      P002|       2|\n",
      "|     104|   Maria Perez|Shelbyville|      P005|       1|\n",
      "|     105| Juan Martinez|Springfield|      P003|       4|\n",
      "|     105| Juan Martinez|Springfield|      P006|       2|\n",
      "|     106|  Elena Torres|Shelbyville|      P001|       2|\n",
      "|     106|  Elena Torres|Shelbyville|      P007|       1|\n",
      "|     107| Pedro Sanchez|Springfield|      P004|       1|\n",
      "|     107| Pedro Sanchez|Springfield|      P005|       3|\n",
      "|     108| Laura Jimenez|Shelbyville|      P002|       1|\n",
      "|     108| Laura Jimenez|Shelbyville|      P006|       2|\n",
      "|     109| Jorge Ramirez|Springfield|      P003|       3|\n",
      "|     109| Jorge Ramirez|Springfield|      P007|       2|\n",
      "|     110| Sofia Alvarez|Shelbyville|      P001|       4|\n",
      "|     110| Sofia Alvarez|Shelbyville|      P005|       1|\n",
      "|     111|Ricardo Torres|Springfield|      P002|       3|\n",
      "+--------+--------------+-----------+----------+--------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Importaciones Necesarias ---\n",
    "from pyspark.sql import SparkSession\n",
    "# 'explode' es la función clave para trabajar con columnas de tipo Array (listas).\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "# --- 2. Creación de la Sesión ---\n",
    "spark = SparkSession.builder.appName(\"DatosComplejos\").getOrCreate()\n",
    "\n",
    "# --- 3. Lectura de Datos Complejos (JSON) ---\n",
    "# Spark puede leer archivos JSON directamente y entender su estructura anidada,\n",
    "# creando columnas de tipo Struct (objetos) y Array (listas).\n",
    "df = spark.read.json(\"datos_pedidos.json\")\n",
    "\n",
    "# Es muy útil imprimir el esquema para entender la estructura que Spark ha inferido.\n",
    "print(\"Esquema inferido del JSON:\")\n",
    "df.printSchema()\n",
    "\n",
    "# truncate=False es útil para ver el contenido completo de las columnas complejas.\n",
    "df.show(truncate=False)\n",
    "\n",
    "# --- 4. Acceso a Datos Anidados ---\n",
    "\n",
    "# Se puede acceder a los campos dentro de una estructura (Struct) usando la notación de punto.\n",
    "print(\"Accediendo a un campo anidado (ciudad):\")\n",
    "df.select(\"order_id\", \"shipping_address.city\").show()\n",
    "\n",
    "# --- 5. \"Explotar\" un Array para Aplanar los Datos ---\n",
    "\n",
    "# La función 'explode' toma una columna de tipo Array ('items') y crea una nueva fila\n",
    "# por cada elemento que contiene el array.\n",
    "df_exploded = df.withColumn(\"item\", explode(\"items\"))\n",
    "\n",
    "print(\"DataFrame después de 'explode' (una fila por cada item del pedido):\")\n",
    "df_exploded.show(truncate=False)\n",
    "\n",
    "# --- 6. Seleccionar los Campos Finales ---\n",
    "\n",
    "# Ahora que cada 'item' está en su propia fila, podemos acceder a sus campos\n",
    "# internos (product_id, quantity) también con la notación de punto.\n",
    "# Este proceso completo se conoce como \"aplanar\" la estructura de datos.\n",
    "df_final = df_exploded.select(\n",
    "    \"order_id\",\n",
    "    \"customer_name\",\n",
    "    \"shipping_address.city\",\n",
    "    \"item.product_id\",\n",
    "    \"item.quantity\"\n",
    ")\n",
    "\n",
    "print(\"Resultado final y aplanado:\")\n",
    "df_final.show()\n",
    "\n",
    "# --- 7. Guardar el Resultado ---\n",
    "# Guardamos el DataFrame aplanado en los formatos deseados.\n",
    "df_final.write.mode(\"overwrite\").parquet(\"pedidos_a_planar_parquet\")\n",
    "df_final.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"pedidos_a_planar_csv\")\n",
    "\n",
    "# --- 8. Finalización ---\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdf57f2",
   "metadata": {},
   "source": [
    "# 9. EJEMPLO: Dataframe de estudiantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6f49524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- enrollment_year: long (nullable = true)\n",
      " |-- extracurricular_activities: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- grades: struct (nullable = true)\n",
      " |    |-- parcial_1: double (nullable = true)\n",
      " |    |-- parcial_2: double (nullable = true)\n",
      " |    |-- parcial_3: double (nullable = true)\n",
      " |    |-- parcial_4: double (nullable = true)\n",
      " |    |-- parcial_5: double (nullable = true)\n",
      " |    |-- parcial_6: double (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- student_id: string (nullable = true)\n",
      "\n",
      "+---+---------------+--------------------------+----------+---------+--------------------------------+---------+----------+\n",
      "|age|enrollment_year|extracurricular_activities|first_name|gender   |grades                          |last_name|student_id|\n",
      "+---+---------------+--------------------------+----------+---------+--------------------------------+---------+----------+\n",
      "|16 |2023           |[Debate, Voleibol]        |Sofía     |Femenino |{8.5, 9.0, 8.8, 9.2, 8.9, 9.5}  |García   |1001      |\n",
      "|17 |2022           |[Fútbol, Música]          |Mateo     |Masculino|{7.8, 8.2, 8.0, 7.5, 8.5, 8.1}  |Hernández|1002      |\n",
      "|16 |2023           |[Teatro]                  |Valentina |Femenino |{9.2, 9.5, 9.0, 9.8, 9.4, 9.7}  |Martínez |1003      |\n",
      "|18 |2021           |[Ajedrez, Ciencia]        |Santiago  |Masculino|{9.8, 9.9, 10.0, 9.7, 9.8, 10.0}|López    |1004      |\n",
      "|17 |2022           |[Baloncesto]              |Isabella  |Femenino |{8.0, 7.9, 8.5, 8.2, 8.8, 8.4}  |González |1005      |\n",
      "|16 |2023           |NULL                      |Sebastián |Masculino|{7.2, 7.0, 7.5, 6.8, 7.1, 7.4}  |Pérez    |1006      |\n",
      "|17 |2022           |[Arte, Fotografía]        |Camila    |Femenino |{9.5, 9.1, 9.3, 9.0, 9.4, 9.6}  |Ramírez  |1007      |\n",
      "|18 |2021           |[Fútbol]                  |Leonardo  |Masculino|{8.8, 8.5, 9.0, 8.6, 8.9, 9.1}  |Flores   |1008      |\n",
      "|16 |2023           |[Danza]                   |Regina    |Femenino |{8.9, 8.7, 9.1, 9.3, 9.0, 9.4}  |Gómez    |1009      |\n",
      "|17 |2022           |[Robótica]                |Daniel    |Masculino|{9.1, 9.3, 8.9, 9.5, 9.2, 9.6}  |Díaz     |1010      |\n",
      "|16 |2023           |[Voleibol]                |Ximena    |Femenino |{8.3, 8.1, 8.5, 8.8, 8.6, 8.9}  |Vázquez  |1011      |\n",
      "|18 |2021           |NULL                      |Alejandro |Masculino|{7.0, 6.5, 7.2, 6.9, 7.5, 7.1}  |Reyes    |1012      |\n",
      "|17 |2022           |[Música, Teatro]          |Mariana   |Femenino |{9.6, 9.4, 9.8, 9.5, 9.7, 9.9}  |Sánchez  |1013      |\n",
      "|16 |2023           |[Baloncesto]              |Matías    |Masculino|{8.1, 8.4, 8.0, 8.3, 8.5, 8.2}  |Torres   |1014      |\n",
      "|17 |2022           |[Debate]                  |Valeria   |Femenino |{9.0, 9.2, 8.8, 9.1, 9.3, 9.4}  |Rojas    |1015      |\n",
      "|18 |2021           |[Ajedrez]                 |Emiliano  |Masculino|{9.4, 9.6, 9.2, 9.7, 9.5, 9.8}  |Cruz     |1016      |\n",
      "|16 |2023           |[Fútbol]                  |Daniela   |Femenino |{8.6, 8.8, 8.4, 8.9, 8.7, 9.0}  |Morales  |1017      |\n",
      "|17 |2022           |[Ciencia]                 |Adrián    |Masculino|{9.7, 9.5, 9.9, 9.6, 9.8, 9.9}  |Ortiz    |1018      |\n",
      "|16 |2023           |NULL                      |Lucía     |Femenino |{7.5, 7.8, 7.2, 7.9, 7.6, 8.0}  |Gutiérrez|1019      |\n",
      "|18 |2021           |[Música]                  |Javier    |Masculino|{8.4, 8.6, 8.2, 8.8, 8.5, 8.9}  |Jiménez  |1020      |\n",
      "+---+---------------+--------------------------+----------+---------+--------------------------------+---------+----------+\n",
      "only showing top 20 rows\n",
      "+---------+-----+\n",
      "|   gender|count|\n",
      "+---------+-----+\n",
      "| Femenino|   15|\n",
      "|Masculino|   15|\n",
      "+---------+-----+\n",
      "\n",
      "+---+---------------+--------------------------+----------+---------+--------------------+---------+----------+\n",
      "|age|enrollment_year|extracurricular_activities|first_name|   gender|              grades|last_name|student_id|\n",
      "+---+---------------+--------------------------+----------+---------+--------------------+---------+----------+\n",
      "| 17|           2022|          [Fútbol, Música]|     Mateo|Masculino|{7.8, 8.2, 8.0, 7...|Hernández|      1002|\n",
      "| 18|           2021|        [Ajedrez, Ciencia]|  Santiago|Masculino|{9.8, 9.9, 10.0, ...|    López|      1004|\n",
      "| 16|           2023|                      NULL| Sebastián|Masculino|{7.2, 7.0, 7.5, 6...|    Pérez|      1006|\n",
      "| 18|           2021|                  [Fútbol]|  Leonardo|Masculino|{8.8, 8.5, 9.0, 8...|   Flores|      1008|\n",
      "| 17|           2022|                [Robótica]|    Daniel|Masculino|{9.1, 9.3, 8.9, 9...|     Díaz|      1010|\n",
      "| 18|           2021|                      NULL| Alejandro|Masculino|{7.0, 6.5, 7.2, 6...|    Reyes|      1012|\n",
      "| 16|           2023|              [Baloncesto]|    Matías|Masculino|{8.1, 8.4, 8.0, 8...|   Torres|      1014|\n",
      "| 18|           2021|                 [Ajedrez]|  Emiliano|Masculino|{9.4, 9.6, 9.2, 9...|     Cruz|      1016|\n",
      "| 17|           2022|                 [Ciencia]|    Adrián|Masculino|{9.7, 9.5, 9.9, 9...|    Ortiz|      1018|\n",
      "| 18|           2021|                  [Música]|    Javier|Masculino|{8.4, 8.6, 8.2, 8...|  Jiménez|      1020|\n",
      "| 16|           2023|                  [Fútbol]|   Ricardo|Masculino|{7.9, 8.1, 7.8, 8...| Castillo|      1022|\n",
      "| 17|           2022|                      NULL|     Diego|Masculino|{6.8, 7.1, 6.5, 7...|   Romero|      1024|\n",
      "| 18|           2021|      [Baloncesto, Robó...|    Samuel|Masculino|{9.2, 9.5, 8.9, 9...|   Juárez|      1026|\n",
      "| 16|           2023|                 [Ajedrez]|  Benjamín|Masculino|{9.9, 9.7, 10.0, ...|    Ramos|      1028|\n",
      "| 18|           2021|         [Fútbol, Ciencia]|    Felipe|Masculino|{9.0, 8.8, 9.2, 8...|   Medina|      1030|\n",
      "+---+---------------+--------------------------+----------+---------+--------------------+---------+----------+\n",
      "\n",
      "+---+---------------+--------------------------+----------+--------+--------------------+---------+----------+\n",
      "|age|enrollment_year|extracurricular_activities|first_name|  gender|              grades|last_name|student_id|\n",
      "+---+---------------+--------------------------+----------+--------+--------------------+---------+----------+\n",
      "| 16|           2023|        [Debate, Voleibol]|     Sofía|Femenino|{8.5, 9.0, 8.8, 9...|   García|      1001|\n",
      "| 16|           2023|                  [Teatro]| Valentina|Femenino|{9.2, 9.5, 9.0, 9...| Martínez|      1003|\n",
      "| 17|           2022|              [Baloncesto]|  Isabella|Femenino|{8.0, 7.9, 8.5, 8...| González|      1005|\n",
      "| 17|           2022|        [Arte, Fotografía]|    Camila|Femenino|{9.5, 9.1, 9.3, 9...|  Ramírez|      1007|\n",
      "| 16|           2023|                   [Danza]|    Regina|Femenino|{8.9, 8.7, 9.1, 9...|    Gómez|      1009|\n",
      "| 16|           2023|                [Voleibol]|    Ximena|Femenino|{8.3, 8.1, 8.5, 8...|  Vázquez|      1011|\n",
      "| 17|           2022|          [Música, Teatro]|   Mariana|Femenino|{9.6, 9.4, 9.8, 9...|  Sánchez|      1013|\n",
      "| 17|           2022|                  [Debate]|   Valeria|Femenino|{9.0, 9.2, 8.8, 9...|    Rojas|      1015|\n",
      "| 16|           2023|                  [Fútbol]|   Daniela|Femenino|{8.6, 8.8, 8.4, 8...|  Morales|      1017|\n",
      "| 16|           2023|                      NULL|     Lucía|Femenino|{7.5, 7.8, 7.2, 7...|Gutiérrez|      1019|\n",
      "| 17|           2022|          [Voleibol, Arte]|  Fernanda|Femenino|{9.3, 9.0, 9.5, 9...|  Mendoza|      1021|\n",
      "| 18|           2021|           [Teatro, Danza]|    Renata|Femenino|{9.1, 9.4, 9.0, 9...|    Silva|      1023|\n",
      "| 16|           2023|              [Fotografía]|     Abril|Femenino|{8.7, 8.9, 8.5, 9...|  Navarro|      1025|\n",
      "| 17|           2022|                  [Debate]|  Victoria|Femenino|{9.4, 9.1, 9.6, 9...| Guerrero|      1027|\n",
      "| 17|           2022|                  [Música]|   Julieta|Femenino|{8.2, 8.5, 8.1, 8...|  Herrera|      1029|\n",
      "+---+---------------+--------------------------+----------+--------+--------------------+---------+----------+\n",
      "\n",
      "+---+---------------+--------------------------+----------+---------+--------------------+---------+----------+--------+\n",
      "|age|enrollment_year|extracurricular_activities|first_name|   gender|              grades|last_name|student_id|promedio|\n",
      "+---+---------------+--------------------------+----------+---------+--------------------+---------+----------+--------+\n",
      "| 17|           2022|          [Fútbol, Música]|     Mateo|Masculino|{7.8, 8.2, 8.0, 7...|Hernández|      1002|    8.02|\n",
      "| 18|           2021|        [Ajedrez, Ciencia]|  Santiago|Masculino|{9.8, 9.9, 10.0, ...|    López|      1004|    9.87|\n",
      "| 16|           2023|                      NULL| Sebastián|Masculino|{7.2, 7.0, 7.5, 6...|    Pérez|      1006|    7.17|\n",
      "| 18|           2021|                  [Fútbol]|  Leonardo|Masculino|{8.8, 8.5, 9.0, 8...|   Flores|      1008|    8.82|\n",
      "| 17|           2022|                [Robótica]|    Daniel|Masculino|{9.1, 9.3, 8.9, 9...|     Díaz|      1010|    9.27|\n",
      "| 18|           2021|                      NULL| Alejandro|Masculino|{7.0, 6.5, 7.2, 6...|    Reyes|      1012|    7.03|\n",
      "| 16|           2023|              [Baloncesto]|    Matías|Masculino|{8.1, 8.4, 8.0, 8...|   Torres|      1014|    8.25|\n",
      "| 18|           2021|                 [Ajedrez]|  Emiliano|Masculino|{9.4, 9.6, 9.2, 9...|     Cruz|      1016|    9.53|\n",
      "| 17|           2022|                 [Ciencia]|    Adrián|Masculino|{9.7, 9.5, 9.9, 9...|    Ortiz|      1018|    9.73|\n",
      "| 18|           2021|                  [Música]|    Javier|Masculino|{8.4, 8.6, 8.2, 8...|  Jiménez|      1020|    8.57|\n",
      "| 16|           2023|                  [Fútbol]|   Ricardo|Masculino|{7.9, 8.1, 7.8, 8...| Castillo|      1022|    8.08|\n",
      "| 17|           2022|                      NULL|     Diego|Masculino|{6.8, 7.1, 6.5, 7...|   Romero|      1024|    6.92|\n",
      "| 18|           2021|      [Baloncesto, Robó...|    Samuel|Masculino|{9.2, 9.5, 8.9, 9...|   Juárez|      1026|    9.28|\n",
      "| 16|           2023|                 [Ajedrez]|  Benjamín|Masculino|{9.9, 9.7, 10.0, ...|    Ramos|      1028|    9.88|\n",
      "| 18|           2021|         [Fútbol, Ciencia]|    Felipe|Masculino|{9.0, 8.8, 9.2, 8...|   Medina|      1030|    9.05|\n",
      "+---+---------------+--------------------------+----------+---------+--------------------+---------+----------+--------+\n",
      "\n",
      "+---+---------------+--------------------------+----------+--------+--------------------+---------+----------+--------+\n",
      "|age|enrollment_year|extracurricular_activities|first_name|  gender|              grades|last_name|student_id|promedio|\n",
      "+---+---------------+--------------------------+----------+--------+--------------------+---------+----------+--------+\n",
      "| 16|           2023|        [Debate, Voleibol]|     Sofía|Femenino|{8.5, 9.0, 8.8, 9...|   García|      1001|    8.98|\n",
      "| 16|           2023|                  [Teatro]| Valentina|Femenino|{9.2, 9.5, 9.0, 9...| Martínez|      1003|    9.43|\n",
      "| 17|           2022|              [Baloncesto]|  Isabella|Femenino|{8.0, 7.9, 8.5, 8...| González|      1005|     8.3|\n",
      "| 17|           2022|        [Arte, Fotografía]|    Camila|Femenino|{9.5, 9.1, 9.3, 9...|  Ramírez|      1007|    9.32|\n",
      "| 16|           2023|                   [Danza]|    Regina|Femenino|{8.9, 8.7, 9.1, 9...|    Gómez|      1009|    9.07|\n",
      "| 16|           2023|                [Voleibol]|    Ximena|Femenino|{8.3, 8.1, 8.5, 8...|  Vázquez|      1011|    8.53|\n",
      "| 17|           2022|          [Música, Teatro]|   Mariana|Femenino|{9.6, 9.4, 9.8, 9...|  Sánchez|      1013|    9.65|\n",
      "| 17|           2022|                  [Debate]|   Valeria|Femenino|{9.0, 9.2, 8.8, 9...|    Rojas|      1015|    9.13|\n",
      "| 16|           2023|                  [Fútbol]|   Daniela|Femenino|{8.6, 8.8, 8.4, 8...|  Morales|      1017|    8.73|\n",
      "| 16|           2023|                      NULL|     Lucía|Femenino|{7.5, 7.8, 7.2, 7...|Gutiérrez|      1019|    7.67|\n",
      "| 17|           2022|          [Voleibol, Arte]|  Fernanda|Femenino|{9.3, 9.0, 9.5, 9...|  Mendoza|      1021|    9.33|\n",
      "| 18|           2021|           [Teatro, Danza]|    Renata|Femenino|{9.1, 9.4, 9.0, 9...|    Silva|      1023|    9.25|\n",
      "| 16|           2023|              [Fotografía]|     Abril|Femenino|{8.7, 8.9, 8.5, 9...|  Navarro|      1025|    8.83|\n",
      "| 17|           2022|                  [Debate]|  Victoria|Femenino|{9.4, 9.1, 9.6, 9...| Guerrero|      1027|    9.43|\n",
      "| 17|           2022|                  [Música]|   Julieta|Femenino|{8.2, 8.5, 8.1, 8...|  Herrera|      1029|    8.43|\n",
      "+---+---------------+--------------------------+----------+--------+--------------------+---------+----------+--------+\n",
      "\n",
      "+---+---------------+----------+---------+--------------------------------+---------+----------+--------+-----------+\n",
      "|age|enrollment_year|first_name|gender   |grades                          |last_name|student_id|promedio|actividades|\n",
      "+---+---------------+----------+---------+--------------------------------+---------+----------+--------+-----------+\n",
      "|17 |2022           |Mateo     |Masculino|{7.8, 8.2, 8.0, 7.5, 8.5, 8.1}  |Hernández|1002      |8.02    |Fútbol     |\n",
      "|17 |2022           |Mateo     |Masculino|{7.8, 8.2, 8.0, 7.5, 8.5, 8.1}  |Hernández|1002      |8.02    |Música     |\n",
      "|18 |2021           |Santiago  |Masculino|{9.8, 9.9, 10.0, 9.7, 9.8, 10.0}|López    |1004      |9.87    |Ajedrez    |\n",
      "|18 |2021           |Santiago  |Masculino|{9.8, 9.9, 10.0, 9.7, 9.8, 10.0}|López    |1004      |9.87    |Ciencia    |\n",
      "|18 |2021           |Leonardo  |Masculino|{8.8, 8.5, 9.0, 8.6, 8.9, 9.1}  |Flores   |1008      |8.82    |Fútbol     |\n",
      "|17 |2022           |Daniel    |Masculino|{9.1, 9.3, 8.9, 9.5, 9.2, 9.6}  |Díaz     |1010      |9.27    |Robótica   |\n",
      "|16 |2023           |Matías    |Masculino|{8.1, 8.4, 8.0, 8.3, 8.5, 8.2}  |Torres   |1014      |8.25    |Baloncesto |\n",
      "|18 |2021           |Emiliano  |Masculino|{9.4, 9.6, 9.2, 9.7, 9.5, 9.8}  |Cruz     |1016      |9.53    |Ajedrez    |\n",
      "|17 |2022           |Adrián    |Masculino|{9.7, 9.5, 9.9, 9.6, 9.8, 9.9}  |Ortiz    |1018      |9.73    |Ciencia    |\n",
      "|18 |2021           |Javier    |Masculino|{8.4, 8.6, 8.2, 8.8, 8.5, 8.9}  |Jiménez  |1020      |8.57    |Música     |\n",
      "|16 |2023           |Ricardo   |Masculino|{7.9, 8.1, 7.8, 8.3, 8.0, 8.4}  |Castillo |1022      |8.08    |Fútbol     |\n",
      "|18 |2021           |Samuel    |Masculino|{9.2, 9.5, 8.9, 9.4, 9.1, 9.6}  |Juárez   |1026      |9.28    |Baloncesto |\n",
      "|18 |2021           |Samuel    |Masculino|{9.2, 9.5, 8.9, 9.4, 9.1, 9.6}  |Juárez   |1026      |9.28    |Robótica   |\n",
      "|16 |2023           |Benjamín  |Masculino|{9.9, 9.7, 10.0, 9.8, 9.9, 10.0}|Ramos    |1028      |9.88    |Ajedrez    |\n",
      "|18 |2021           |Felipe    |Masculino|{9.0, 8.8, 9.2, 8.9, 9.1, 9.3}  |Medina   |1030      |9.05    |Fútbol     |\n",
      "|18 |2021           |Felipe    |Masculino|{9.0, 8.8, 9.2, 8.9, 9.1, 9.3}  |Medina   |1030      |9.05    |Ciencia    |\n",
      "+---+---------------+----------+---------+--------------------------------+---------+----------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Importaciones y Creación de la Sesión ---\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Crear la sesión de Spark, que es el punto de entrada a toda su funcionalidad.\n",
    "spark = SparkSession.builder.appName(\"estudiantes\").getOrCreate()\n",
    "\n",
    "# --- 2. Carga y Exploración de Datos ---\n",
    "# Obtener los datos de un archivo JSON. Spark infiere automáticamente el esquema complejo.\n",
    "df_estudiantes = spark.read.json(\"estudiantes.json\")\n",
    "\n",
    "# Mostrar el esquema del DataFrame para entender su estructura (columnas, tipos y si son anidados).\n",
    "df_estudiantes.printSchema()\n",
    "\n",
    "# Mostrar el contenido del DataFrame. truncate=False evita que el contenido de las columnas se corte.\n",
    "df_estudiantes.show(truncate=False)\n",
    "\n",
    "# --- 3. Agrupación y Filtrado ---\n",
    "# Agrupar por la columna 'gender' y contar cuántas filas (estudiantes) hay en cada grupo.\n",
    "df_estudiantes.groupBy(\"gender\").count().show()\n",
    "\n",
    "# Filtrar el DataFrame para crear uno nuevo que solo contenga a los hombres.\n",
    "df_hombres = df_estudiantes.filter(df_estudiantes.gender == \"Masculino\")\n",
    "df_hombres.show()\n",
    "\n",
    "# Filtrar para crear un DataFrame que solo contenga a las mujeres.\n",
    "df_mujeres = df_estudiantes.filter(df_estudiantes.gender == \"Femenino\")\n",
    "df_mujeres.show()\n",
    "\n",
    "# --- 4. Cálculo de Promedios Accediendo a Datos Anidados ---\n",
    "# Importar las funciones 'round' para redondear y 'col' para referenciar columnas.\n",
    "from pyspark.sql.functions import round, col\n",
    "\n",
    "# Añadir una nueva columna \"promedio\" al DataFrame de hombres.\n",
    "df_hombres_calif = df_hombres.withColumn(\"promedio\", \n",
    "    # Usamos round() para redondear el resultado final a 2 decimales.\n",
    "    round(\n",
    "        # Se accede a cada calificación anidada con la notación de punto (ej: \"grades.parcial_1\").\n",
    "        # Se suman todas las calificaciones y se dividen entre 6 para obtener el promedio.\n",
    "        ((col(\"grades.parcial_1\") + col(\"grades.parcial_2\") + col(\"grades.parcial_3\") +\n",
    "          col(\"grades.parcial_4\") + col(\"grades.parcial_5\") + col(\"grades.parcial_6\")) / 6),\n",
    "        2  # Número de decimales para el redondeo.\n",
    "    )\n",
    ")\n",
    "df_hombres_calif.show()\n",
    "\n",
    "# Se repite el mismo cálculo para el DataFrame de mujeres.\n",
    "df_mujeres_calif = df_mujeres.withColumn(\"promedio\",\n",
    "    round(\n",
    "        ((col(\"grades.parcial_1\") + col(\"grades.parcial_2\") + col(\"grades.parcial_3\") + \n",
    "          col(\"grades.parcial_4\") + col(\"grades.parcial_5\") + col(\"grades.parcial_6\")) / 6),\n",
    "        2\n",
    "    )\n",
    ")\n",
    "df_mujeres_calif.show()\n",
    "\n",
    "# --- 5. Aplanar Datos con 'explode' ---\n",
    "# Importar la función 'explode' para trabajar con columnas de tipo array.\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "# 'explode' crea una nueva fila por cada elemento en el array 'extracurricular_activities'.\n",
    "# La nueva columna se llamará 'actividades'.\n",
    "df_hombres_exploded = df_hombres_calif.withColumn(\"actividades\", explode(\"extracurricular_activities\"))\n",
    "\n",
    "# Eliminar la columna original del array para evitar datos redundantes.\n",
    "df_hombres_exploded = df_hombres_exploded.drop(\"extracurricular_activities\")\n",
    "\n",
    "# Mostrar el resultado final aplanado.\n",
    "df_hombres_exploded.show(truncate=False)\n",
    "\n",
    "# --- 6. Finalización ---\n",
    "# Detener la sesión de Spark para liberar los recursos.\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
